
# XKubeMesh orchestrate XKube objects, where each xkube object 
# is a Kubernetes resource. 
# 
# XKube object must create resources that are associated with xkube 
# at the time of creation. This includes secrets containing kubeconfig,
# ssh resources required for configuration, etc. 
#
# XKubeMesh utilizes each cluster's data, obtained from XKube object
# to configure inter-cluster coordination.
#
# Extra resources:
#  - Secret (k3s token)
#  - ProviderConfig (k3s cluster)
#
#
# List of objects:
#  - XKube (load Xkubes with its Secret containing kubeconfig)
#  - Object [Job] (Istio CA certificates and remote secrets)
#  - Object [Secret] Apply remote secrets to each cluster using ProviderConfig
#  - Helm [istio-base]
#  - Helm [Submariner] # TODO: this may be happen in the XKube?
#
#  Note: clusterName is the name of xkubes.skycluster.io
#        we need the xkubes.<cloud>.skycluter.io available within status.clusterName
#
#  List all xkube clusters (assume names are given by user)
#
#  Construct Job to generates certificates and remote secrets for all clusters (it automatically 
#    fetches the secrets by labels (skycluster.io/secret-type=k8s-connection-data)
#
#  Once done successfully (conditional), get all generated secrets (using labels
#    skycluster.io/secret-type=cluster-cacert, compare label
#    skycluster.io/cluster-name=<cluster-name>) with the names of xkube objects
#
#  Need to fetch providerconfigs.kuberenetes for all xkubes clusters    
#
#  Using kubernetes providerconfigs, generates remote secret of each cluster on 
#  all other clusters.
#
#  Creates two map: <cluster-name: []provider-cfgs> and 
#    <cluster-name>: []remote-secrets and use them below:
#
# {
#   object-kubernetes-secret{
#     remote_secret_for_s
#     providercfg_remote_t
#   } for s in cluster-names for t in cluster-names if s != t 
# }

import yaml
import json
import base64
import helper.v1alpha1.main as helper
import provider_kubernetes.v1alpha2 as k8sv1a2
import provider_helm.v1beta1 as helmv1b1

oxr = option("params").oxr # observed composite resource
ocds = option("params")?.ocds # observed composed resources
extra = option("params")?.extraResources
# _dxr = option("params").dxr # desired composite resource
# dcds = option("params").dcds # desired composed resources

# _oxrProvPlatform = oxr.spec?.providerRef?.platform or Undefined
# _oxrProvRegion = oxr.spec?.providerRef?.region or Undefined
# _oxrProvZones = oxr.spec?.providerRef?.zones or Undefined
# _oxrProvZone = oxr.spec?.providerRef?.zones?.primary or Undefined
# _oxrAppId = oxr.spec?.applicationId or Undefined

ctx = option("params")?.ctx
assert ctx is not Undefined, "Context must be provided in the params"

_extraRes = ctx["apiextensions.crossplane.io/extra-resources"]
assert _extraRes is not Undefined, "Extra resources must be provided in the context"

_xsetup = _extraRes["XSetups"]?[0] or Undefined
assert _xsetup is not Undefined, "XSetup resource must be provided in the extra resources"

# These are secrets containing istio remote secrets for remote clusters
# They will be generated by the Job later here
_remoteSecrets = _extraRes["ClusterRemoteSecrets"] or Undefined
# assert _remoteSecrets is not Undefined, "ClusterRemoteSecrets resource must be provided in the extra resources"

_remoteK8SProviderCfgs = _extraRes["ClusterK8SProviderConfigs"] or Undefined
assert _remoteK8SProviderCfgs is not Undefined, "ClusterK8SProviderConfigs resource must be provided in the extra resources"

_remoteHelmProviderCfgs = _extraRes["ClusterHelmProviderConfigs"] or Undefined
assert _remoteHelmProviderCfgs is not Undefined, "ClusterHelmProviderConfigs resource must be provided in the extra resources"

assert oxr.spec?.localCluster?.podCidr, "Pod CIDR must be provided in the spec"
assert oxr.spec?.localCluster?.serviceCidr, "Service CIDR must be provided in the spec"

# Default (local) provider config
_k8sProvCfgName = _xsetup.status?.providerConfig?.kubernetes?.name or Undefined
_k8sMgmtClusterName = "k8s-skycluster-management"

_default_labels = {
  **oxr.metadata?.labels,
  "skycluster.io/managed-by" = "skycluster"
}

_default_annotations = {**oxr.metadata?.annotations}

_clusterNames = oxr.spec?.clusterNames or []
assert len(_clusterNames) > 0, "At least one cluster name must be provided in the spec.clusters"

#
# fetch referenced xkubes objects based on the cluster names (cloud-provider specific k8s)
#
_clusterNamesRef = [o?.Resource?.status?.clusterName for s in _clusterNames for k, obj in extra if k == s for o in obj]
# since we have the local management cluster as part of the multi-cluster setup
# we manually add the name of the local management cluster
_clusterNamesRef += [_k8sMgmtClusterName]

# construct helper maps for remote-secrets and remote-provider-configs
_remoteSecretsMap: {str:{str:any}} = {
  cn = {
      data = s.data
      name = s.metadata?.name
    } for cn in _clusterNamesRef for s in _remoteSecrets \
      if cn == s.metadata?.labels["skycluster.io/cluster-name"]
} if _remoteSecrets else Undefined

_remoteK8SKubeconfigs = {
  cn = {
    kubeconfig = p?.Resource?.data?["kubeconfig"]
  } for cn in _clusterNamesRef for p in (extra?["kconfigSecrets"] or []) \
      if cn == p?.Resource?.metadata?.labels["skycluster.io/cluster-name"]
}

_remoteK8SProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteK8SProviderCfgs \
      if cn == p.metadata?.labels["skycluster.io/cluster-name"]
}

_remoteHelmProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteHelmProviderCfgs \
      if cn == p.metadata?.labels["skycluster.io/cluster-name"]
}
assert len(_remoteK8SProviderCfgsMap) == len(_remoteHelmProviderCfgsMap), \
  "Remote K8S and Helm provider configs must exist for all clusters, found {} K8S and {} Helm configs."\
    .format(len(_remoteK8SProviderCfgsMap), len(_remoteHelmProviderCfgsMap))

# merge the two
_remoteProviderCfgsMaps = {
  cn = {
    k8s = _remoteK8SProviderCfgsMap[cn]
    helm = _remoteHelmProviderCfgsMap[cn]
  } for cn in _remoteHelmProviderCfgsMap
}

# Retrieve xkubes pod and service cidr
# will be used in the next step
_xkubesRefData = {
  o?.Resource?.status?.clusterName = {
    podCidr = o?.Resource?.status?.podCidr
    serviceCidr = o?.Resource?.status?.serviceCidr
  } for s in _clusterNames for k, obj in extra if k == s for o in obj
} | {
  # merge with local management cluster's data
  k = {
    podCidr = oxr.spec?.localCluster?.podCidr
    serviceCidr = oxr.spec?.localCluster?.serviceCidr
  } for k in [_k8sMgmtClusterName]
}



_items = []

#
# Ensure namespace istio-system and skycluster-system exists in the remote clusters and local
# Note: provider config for local cluster is retrieved by same filter:
# skycluster.io/config-type = k8s-connection-data through the extraResources
#
_items += [
  k8sv1a2.Object{
    metadata = {
      annotations =  helper._set_resource_name("ns{}{}".format(i,ns))
    }
    spec = {
      deletionPolicy = "Orphan"
      forProvider.manifest = {
        apiVersion = "v1"
        kind = "Namespace"
        metadata = {
          name = ns
        }
      }
      providerConfigRef.name = pc
    }
  } for i, pc in [_remoteK8SProviderCfgsMap[o] for o in _clusterNamesRef] for ns in ["istio-system", "skycluster-system", "submariner-operator"]
] 


#
# Pod Generating Headscale Certificate
#
# This job creates a POD that generates secrets for each cluster. The generated secret
# contains generated CA certificates for the cluster and the remote-secret
# used by istio, stored in a secret with name pattern of <secret-name>-cacerts,
# where <secret-name> is the name of secret that contains kubeconfig data for the cluster.
#
# The generated secrets are identifiable by label 
#   "skycluster.io/secret-type=istio-cluster-cacert"
# and has a reference to its secret by
#   "skycluster.io/cluster-name=<secret-name>"
#
# The generated secrets are stored in skycluster-system namespace
# but need to be applied to the target namespace of istio-system
# when they are created in the remote clusters.
#


#
# Job: generating ca certs for each cluster, it generates certs based on secrets
# filtered by skycluster.io/secret-type=k8s-connection-data
#
_items += [
  k8sv1a2.Object{
    metadata = {
      labels = _default_labels | {"skycluster.io/job-type": "istio-cluster-ca-certs"}
      annotations = _default_annotations | helper._set_resource_name("caCerts")
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "batch/v1"
        kind = "Job"
        metadata = { 
          name = "istio-cluster-ca-certs-generator",
          namespace = "skycluster-system",
          labels = _default_labels | {"skycluster.io/job-type": "istio-cluster-ca-certs"},
          annotations = _default_annotations
        }
        spec = {
          backoffLimit = 0
          ttlSecondsAfterFinished = 300
          template = {
            metadata.labels = _default_labels |{
              "skycluster.io/job-type": "istio-cluster-ca-certs"
            }
            spec = {
              serviceAccountName: "skycluster-sva"
              restartPolicy = "Never"
              volumes = [{
                  name = "work"
                  emptyDir = {}
                }, {
                  name = "script"
                  configMap = {
                    name = "script-istio-cluster-certs"
                    defaultMode = 0755
                  }
                }
              ]
              containers = [{
                  name = "runner"
                  image = "etesami/kubectl:latest"
                  imagePullPolicy = "IfNotPresent"
                  command = ["/bin/bash","-lc","/script/run.sh"]
                  env = [{
                      name = "NAMESPACE"
                      value = "skycluster-system"
                    }, {
                      name = "ROOT_SECRET_NAME"
                      value = "istio-root-ca"
                    }, {
                      name = "KCFG_SELECTOR"
                      value = "skycluster.io/managed-by=skycluster,skycluster.io/secret-type=k8s-connection-data"
                    }
                  ]
                  volumeMounts = [{
                      name = "work"
                      mountPath = "/work"
                    }, {
                      name = "script"
                      mountPath = "/script"
                    }
                  ]
                }
              ]
            }
          }
        }
      }
      providerConfigRef = {
        name = _k8sProvCfgName
      }
    }
  }
] if _k8sProvCfgName or ocds?["caCerts"] else []

_caCertPodStatus = ocds?["caCerts"]?.Resource?.status?.atProvider?.manifest?.status?.succeeded

#
# Install Istio on remote clusters
#
# Dealing with namespace creation is tricky, as we need to ensure that the namespace exists
# before we can install Istio. 
#
# TOBE CHECKED:
#  In my experience, "skipCreateNamespace" is not reliable
#  to use since if it is set to True, the namespace is not created if it does not exist, 
#  if it is set to False, the installation may fail if cannot manage the existing ones.
#

#
# [K8S Object] Installing istio cacerts on each local cluster
#
_items += [
  # TODO: may need to introduce clean-up job
  k8sv1a2.Object{
    metadata = {
      labels = _default_labels | {"skycluster.io/secret-type": "remote"}
      annotations = _default_annotations | helper._set_resource_name("caCerts{}".format(s))
    }
    spec = {
      # TODO: delete or orphan?
      forProvider.manifest = {
        apiVersion = "v1"
        kind = "Secret"
        metadata = {
          name = "cacerts"
          namespace = "istio-system"
          labels = _default_labels
          annotations = _default_annotations
        }
        data = {
          "ca-cert.pem" = _remoteSecretsMap[s]?.data?["ca-cert.pem"]
          "ca-key.pem" = _remoteSecretsMap[s]?.data?["ca-key.pem"]
          "root-cert.pem" = _remoteSecretsMap[s]?.data?["root-cert.pem"]
          "cert-chain.pem" = _remoteSecretsMap[s]?.data?["cert-chain.pem"]
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  } for i, s in _clusterNamesRef
] if _caCertPodStatus and _remoteSecretsMap and _remoteProviderCfgsMaps else []

#
# Istio Base using helm
#
# _chartData = {
#   version = "1.27.0"
#   repo = "https://istio-release.storage.googleapis.com/charts"
#   name = "base"
#   ns = "istio-system"
# }
# _items += [
#   k8sv1a2.Object{
#     metadata = {
#       labels = _default_labels | {"skycluster.io/job-type": "istio-base-clean-up"}
#       annotations = _default_annotations | helper._set_resource_name("istioBaseCleanUp{}".format(s))
#     }
#     spec =  {
#       forProvider.manifest = {
#         apiVersion = "batch/v1"
#         kind = "Job"
#         metadata = { 
#           name = "istiob-cleanup-{}".format(s),
#           namespace = "skycluster-system",
#           labels = _default_labels | {"skycluster.io/job-type": "istio-base-clean-up"},
#           annotations = _default_annotations
#         }
#         spec = {
#           backoffLimit = 0
#           template = {
#             metadata.labels = _default_labels | {
#               "skycluster.io/job-type": "istio-base-clean-up"
#               "skycluster.io/cluster-name": s
#             }
#             spec = {
#               serviceAccountName = "skycluster-sva"
#               restartPolicy = "Never"
#               volumes = [{
#                   name = "work"
#                   emptyDir = {}
#                 }, {
#                   name = "script"
#                   configMap = {
#                     name = "script-helm-clean-up"
#                     defaultMode = 0755
#                   }
#                 }
#               ]
#               containers = [{
#                   name = "runner"
#                   image = "etesami/kubectl:latest"
#                   imagePullPolicy = "Always"
#                   command = ["/bin/bash","-lc","/script/run.sh"]
#                   env = [{
#                       name = "CHART_REPO"
#                       value = _chartData.repo
#                     }, {
#                       name = "CHART_NAME"
#                       value = _chartData.name
#                     }, {
#                       name = "CHART_VERSION"
#                       value = _chartData.version
#                     },{
#                       name = "CHART_NAMESPACE"
#                       value = _chartData.ns
#                     }, {
#                       name = "KUBECONFIG_DATA"
#                       value = _remoteK8SKubeconfigs[s]?["kubeconfig"] or "local"
#                     }, {
#                       # space-separated K8s object references
#                       name = "BLOCKING_OBJECTS"
#                       value = "serviceaccount/istio-reader-service-account CustomResourceDefinition/wasmplugins.extensions.istio.io"
#                     }, {
#                       name = "BLOCKING_CLUSTER_OBJECTS"
#                       value = "crd/.+\\.istio\\.io$ clusterrolebinding/istio-.* clusterrole/istio-.*"
#                     }
#                   ]
#                   volumeMounts = [{
#                       name = "work"
#                       mountPath = "/work"
#                     }, {
#                       name = "script"
#                       mountPath = "/script"
#                     }
#                   ]
#                 }
#               ]
#             }
#           }
#         }
#       }
#       providerConfigRef = {
#         # run the pod in the local cluster and use kubeconfig to configure remote cluster
#         name = _k8sProvCfgName
#       }
#     }
#   } for s in _clusterNamesRef
# ] if _remoteK8SKubeconfigs else []

# _istioCleanUpReady = {
#   s = helper._ready(ocds?["istioBaseCleanUp{}".format(s)]) for s in _clusterNamesRef
# }
# _items += [
#   helmv1b1.Release{
#     metadata = {
#       labels = {
#         **oxr.metadata?.labels,
#         "skycluster.io/managed-by": "skycluster",
#       },
#       annotations = {
#         **oxr.metadata?.annotations,
#         **helper._set_resource_name("istioBase{}".format(s))
#       }
#     }
#     spec = {
#       forProvider = {
#         chart = {
#           name = _chartData.name
#           repository = _chartData.repo
#           version = _chartData.version
#         }
#         namespace = _chartData.ns
#         skipCreateNamespace = False
#       }
#       providerConfigRef = {
#         name = _remoteProviderCfgsMaps?[s]?.helm
#       }
#     }
#   } for s in _clusterNamesRef if _istioCleanUpReady[s]
# ] if _caCertPodStatus and _remoteSecretsMap and _remoteProviderCfgsMaps else []

#
# Istio operator using helm
# #
# _items += [
#   helmv1b1.Release{
#     metadata = {
#       labels = {
#         **oxr.metadata?.labels,
#         "skycluster.io/managed-by": "skycluster",
#       },
#       annotations = {
#         **oxr.metadata?.annotations,
#         **helper._set_resource_name("istiod{}".format(s))
#       }
#     }
#     spec = {
#       forProvider = {
#         chart = {
#           name = "istiod"
#           repository = "https://istio-release.storage.googleapis.com/charts"
#           version = "1.27.0"
#         }
#         set = [
#           {
#             name = "global.meshID"
#             value = "mesh1"
#           },
#           {
#             name = "global.network"
#             value = "network1"
#           },
#           {
#             name = "global.multiCluster.clusterName"
#             value = s
#           }
#         ]
#         namespace = "istio-system"
#         skipCreateNamespace = False
#       }
#       providerConfigRef = {
#         name = _remoteProviderCfgsMaps?[s]?.helm
#       }
#     }
#   } for s in _clusterNamesRef
# ] if _caCertPodStatus and _remoteSecretsMap and _remoteProviderCfgsMaps else []

#
# Apply remote-secret for each cluster on all other clusters
#
# _items += [
#   k8sv1a2.Object{
#     metadata = {
#       labels = _default_labels | {"skycluster.io/secret-type": "remote"}
#       annotations = _default_annotations | helper._set_resource_name("remote{}".format(s)) | {
#         "skycluster.io/from-cluster": s,
#         "skycluster.io/to-cluster": t
#       }
#     }
#     spec = {
#       forProvider.manifest = {
#         apiVersion = "v1"
#         kind = "Secret"
#         metadata = {
#           name = yaml.decode(base64.decode(_remoteSecretsMap[s]?.data?["remote-secret.yaml"]).replace("---",""))?.metadata?.name
#           namespace = yaml.decode(base64.decode(_remoteSecretsMap[s]?.data?["remote-secret.yaml"]).replace("---",""))?.metadata?.namespace
#           labels = yaml.decode(base64.decode(_remoteSecretsMap[s]?.data?["remote-secret.yaml"]).replace("---",""))?.metadata?.labels | _default_labels | {
#             "skycluster.io/secret-type": "remote"
#           }
#           annotations = yaml.decode(base64.decode(_remoteSecretsMap[s]?.data?["remote-secret.yaml"]).replace("---",""))?.metadata?.annotations | _default_annotations
#         }
#         stringData = yaml.decode(base64.decode(_remoteSecretsMap[s]?.data?["remote-secret.yaml"]).replace("---",""))?.stringData
#       }
#       providerConfigRef = {
#         name = _remoteProviderCfgsMaps[t]?.k8s
#       }
#     }
#   } for s in _clusterNamesRef for t in _clusterNamesRef if s != t
# ] if _caCertPodStatus and _remoteSecretsMap and _remoteProviderCfgsMaps else []


#
# Submariner Setup
# Ensure the namespaces is available
#

# _items += [
#   k8sv1a2.Object{
#     metadata = {
#       annotations =  helper._set_resource_name("submNS{}".format(s))
#     }
#     spec = {
#       deletionPolicy = "Orphan"
#       forProvider.manifest = {
#         apiVersion = "v1"
#         kind = "Namespace"
#         metadata = {
#           name = "submariner-operator"
#         }
#       }
#       providerConfigRef.name = _remoteProviderCfgsMaps[s]?.k8s
#     }
#   } for s in _clusterNamesRef
# ] 

#
# Create a secret for Submariner connection on a remote cluster
# Need to get psk, brokerCA, brokerToken and apiServer, using extraResource to fetch them.
#

# Expected only one submarinerSecret
_subSecretData = extra?["submarinerSecret"]?[0]?.Resource?.data?["values.yaml"]
_subSecret = json.decode(base64.decode(_subSecretData)) if _subSecretData else Undefined
_apiServer = _subSecret?.broker?.server
_psk = _subSecret?.ipsec?.psk
_brokerCA = _subSecret?.broker?.ca
_brokerToken = _subSecret?.broker?.token

_subGate = all_true([_caCertPodStatus, _remoteSecretsMap, _remoteProviderCfgsMaps, _apiServer, _brokerCA, _brokerToken, _psk])

#
# Submariner Secret for remote clusters
# The local management cluster already have this secret
#
_items += [
  k8sv1a2.Object{
    metadata = {
      labels = _default_labels
      annotations = _default_annotations | {
        **helper._set_resource_name("submSc{}".format(s)),
      } 
    }
    spec = {
      forProvider.manifest = {
        apiVersion = "v1"
        kind = "Secret"
        metadata = { 
          name = "submariner-connection-secret",
          namespace = "skycluster-system",
          labels = _default_labels | {
            "skycluster.io/config-type": "connection-secret"
          },
          annotations = _default_annotations
        }
        type: "Opaque"
        stringData = {
          "values.yaml" = json.encode({
            "ipsec": {
              "psk": _psk
            },
            "broker": {
              "server": _apiServer,
              "namespace": "skycluster-system",
              "ca" : _brokerCA,
              "token": _brokerToken
            }
          })
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
    # submariner-connection-secret already is setup in the management cluster
  } for s in _clusterNamesRef if s != _k8sMgmtClusterName
] if _subGate else []


#
# Submariner operator, using the secret above
# We already create istio-operator for the local kubernetes cluster
# so we skip creation for the local cluster
#

_submChartData = {
  version = "0.20.1"
  repo = "https://submariner-io.github.io/submariner-charts/charts"
  name = "submariner-operator"
  ns = "submariner-operator"
}
_items += [
  *[
    k8sv1a2.Object{
    metadata = {
      labels = _default_labels | {"skycluster.io/job-type": "subm-cleanup"}
      annotations = _default_annotations | helper._set_resource_name("subm-cleanup-{}".format(s))
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "batch/v1"
        kind = "Job"
        metadata = { 
          name = "subm-cleanup-{}".format(s),
          namespace = "skycluster-system",
          labels = _default_labels | {"skycluster.io/job-type": "subm-cleanup-clean-up"},
          annotations = _default_annotations
        }
        spec = {
          backoffLimit = 0
          ttlSecondsAfterFinished = 120
          template = {
            metadata.labels = _default_labels | {
              "skycluster.io/job-type": "subm-cleanup-clean-up"
              "skycluster.io/cluster-name": s
            }
            spec = {
              serviceAccountName = "skycluster-sva"
              restartPolicy = "Never"
              volumes = [{
                  name = "work"
                  emptyDir = {}
                }, {
                  name = "script"
                  configMap = {
                    name = "script-helm-clean-up"
                    defaultMode = 0755
                  }
                }
              ]
              containers = [{
                  name = "runner"
                  image = "etesami/kubectl:latest"
                  imagePullPolicy = "Always"
                  command = ["/bin/bash","-lc","/script/run.sh"]
                  env = [{
                      name = "CHART_REPO"
                      value = _submChartData.repo
                    }, {
                      name = "CHART_NAME"
                      value = _submChartData.name
                    }, {
                      name = "CHART_VERSION"
                      value = _submChartData.version
                    },{
                      name = "CHART_NAMESPACE"
                      value = _submChartData.ns
                    }, {
                      name = "KUBECONFIG_DATA"
                      value = _remoteK8SKubeconfigs[s]?["kubeconfig"] or "local"
                    }, {
                      # space-separated K8s object references
                      name = "BLOCKING_OBJECTS"
                      value = "Submariner/submariner"
                    }, {
                      name = "CLUSTERROLE_PREFIX"
                      value = "submariner"
                    }
                  ]
                  volumeMounts = [{
                      name = "work"
                      mountPath = "/work"
                    }, {
                      name = "script"
                      mountPath = "/script"
                    }
                  ]
                }
              ]
            }
          }
        }
      }
      providerConfigRef = {
        # run the pod in the local cluster and use kubeconfig to configure remote cluster
        name = _k8sProvCfgName
      }
    }
    # submariner in management cluster is handled by xoverlay API
  } for s in _clusterNamesRef if s != _k8sMgmtClusterName] + [
    # This is a dummy pod that is required for submariner to detect service cidr.
    # Submariner implement a bad practice in determining service cidr where 
    # in our case it fails.
    k8sv1a2.Object{
    metadata = {
      labels = _default_labels | {"skycluster.io/pod-type": "dummy-pod"}
      annotations = _default_annotations | helper._set_resource_name("dummy-pod-{}".format(s))
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "v1"
        kind = "Pod"
        metadata = { 
          name = "dummy-pod-{}".format(s),
          namespace = "kube-system",
          labels = _default_labels | {
            "component": "kube-apiserver"
            "skycluster.io/pod-type": "dummy-pod"
          },
          annotations = _default_annotations
        }
        spec = {
          restartPolicy = "Always"
          containers = [{
            name = "dummy-container"
            image = "busybox"
            command = ["sh","-c","echo \"Starting dummy pod\";\n          tail -f /dev/null"]
            args = ["--service-cluster-ip-range={}".format(_xkubesRefData[s]?.serviceCidr)]  
          }]
        }
      }
      providerConfigRef = {
        # run the pod in the local cluster and use kubeconfig to configure remote cluster
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  } for s in _clusterNamesRef if s != _k8sMgmtClusterName]
] if _remoteK8SKubeconfigs else []

_submCleanUpReady = all_true([
  helper._ready(ocds?["subm-cleanup-{}".format(s)]) for s in _clusterNamesRef if s != _k8sMgmtClusterName
  ] + [
  helper._ready(ocds?["dummy-pod-{}".format(s)]) for s in _clusterNamesRef if s != _k8sMgmtClusterName
])

_items += [
  
  helmv1b1.Release{
    metadata = {
      labels = _default_labels 
      annotations = _default_annotations | {
        **helper._set_resource_name("submCluster{}".format(s))
      }
    }
    spec = {
      forProvider = {
        chart = {
          name = _submChartData.name
          repository = _submChartData.repo
          version = _submChartData.version
        }
        namespace = _submChartData.ns
        skipCreateNamespace = True
        valuesFrom = [{
          secretKeyRef = {
            key = "values.yaml"
            name = "submariner-connection-secret"
            namespace = "skycluster-system"
          }
        }]
        values = {
          debug = "true",
          submariner = {
            serviceDiscovery = "false",
            cableDriver = "wireguard",
            clusterId = s,
            clusterCidr = _xkubesRefData[s]?.podCidr,
            serviceCidr = _xkubesRefData[s]?.serviceCidr,
            natEnabled = "true"
          }
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.helm
      }
    }
    # We don't need to install submariner in the management cluster
  } for s in _clusterNamesRef if s != _k8sMgmtClusterName
] if _submCleanUpReady and _xkubesRefData and _clusterNamesRef else []  


# ###################### dxr ######################

dxr = {
  **option("params").dxr,
  status = {
    clusters = [{ 
      nameRef = s
      providerConfigName = _remoteK8SProviderCfgsMap[s]
      if _remoteSecretsMap:
        secretName = _remoteSecretsMap[s]?.name or Undefined
    } for s in _clusterNamesRef]
    log = json.encode(
      {
        submCleanUpReady = [helper._ready(ocds?["subm-cleanup-{}".format(s)]) for s in _clusterNamesRef]
        submCleanUpReadyV = _submCleanUpReady
        xkubesRefData = _xkubesRefData
        clusterNamesRef = _clusterNamesRef
        # remoteK8SProviderCfgs = _remoteK8SProviderCfgs
        # dummy = "dummy"
        # caCertPodStatus = _caCertPodStatus
        # remoteSecret = [_remoteSecretsMap[s]?.data?["remote-secret.yaml"] for s in _clusterNamesRef][0]
        # remotpc = _remoteProviderCfgsMaps
        # "podStatus" = _caCertPodStatus or "NA",
        # "status" = True if _caCertPodStatus and _remoteSecretsMap and _remoteK8SProviderCfgsMap else False
        # **{cacert = ocds?["caCerts"]}
        # **_remoteSecretsMap,
        # **_remoteProviderCfgsMap,
        # **{k = v for k, v in exr},
        # **{ref = _clusterNamesRef},
        # **_remoteSecretsMap,
        # **{remoteSec = _remoteSecrets}
      }
    )
  }
}

extraItems = {
  apiVersion = "meta.krm.kcl.dev/v1alpha1"
  kind = "ExtraResources"
  requirements = {
    **{s = {
        apiVersion: "skycluster.io/v1alpha1",
        kind: "XKube",
        matchName: s
      } for s in _clusterNames if s != _k8sMgmtClusterName
    }
    **{"submarinerSecret" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/config-type": "connection-secret"
        }
      }
    }
    **{"kconfigSecrets" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/secret-type": "k8s-connection-data"
        }
      }
    }
  }
}

items = [*_items, dxr, extraItems]
