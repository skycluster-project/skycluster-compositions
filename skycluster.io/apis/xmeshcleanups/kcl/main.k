
# XKubeMesh Cleanup

import json

import helper.v1alpha1.main as helper
import provider_kubernetes.v1alpha2 as k8sv1a2

oxr = option("params").oxr # observed composite resource
ocds = option("params")?.ocds # observed composed resources
extra = option("params")?.extraResources
# _dxr = option("params").dxr # desired composite resource
# dcds = option("params").dcds # desired composed resources

ctx = option("params")?.ctx
assert ctx is not Undefined, "Context must be provided in the params"

_extraRes = ctx["apiextensions.crossplane.io/extra-resources"]
assert _extraRes is not Undefined, "Extra resources must be provided in the context"

_xsetup = _extraRes["XSetups"]?[0] or Undefined
assert _xsetup is not Undefined, "XSetup resource must be provided in the extra resources"

_cmsStatus = extra["cmsStatus"] or Undefined

# These are secrets containing istio remote secrets for remote clusters
# They will be generated by the Job later here
_remoteSecrets = extra["clusterRemoteSecrets"] or Undefined

_remoteK8SProviderCfgs = _extraRes["ClusterK8SProviderConfigs"] or Undefined
assert _remoteK8SProviderCfgs is not Undefined, "ClusterK8SProviderConfigs resource must be provided in the extra resources"

_remoteHelmProviderCfgs = _extraRes["ClusterHelmProviderConfigs"] or Undefined
assert _remoteHelmProviderCfgs is not Undefined, "ClusterHelmProviderConfigs resource must be provided in the extra resources"

assert oxr.spec?.localCluster?.podCidr, "Pod CIDR must be provided in the spec"
assert oxr.spec?.localCluster?.serviceCidr, "Service CIDR must be provided in the spec"

# Default (local) provider config
_k8sProvCfgName = _xsetup.status?.providerConfig?.kubernetes?.name or Undefined
_k8sMgmtClusterName = "skycluster-management"

_clusterNames = oxr.spec?.clusterNames or []
assert len(_clusterNames) > 0, "At least one cluster name must be provided in the spec.clusters"

#
# fetch referenced xkubes objects based on the cluster names (cloud-provider specific k8s)
#
_clusterNamesRef = [o?.Resource?.status?.clusterName for s in _clusterNames for k, obj in extra if k == s for o in obj]
# since we have the local management cluster as part of the multi-cluster setup
# we manually add the name of the local management cluster
_clusterNamesRef += [_k8sMgmtClusterName]

# construct helper maps for remote-secrets and remote-provider-configs
_remoteSecretsMap: {str:{str:any}} = {
  cn = {
      data = s.Resource?.data
      name = s.Resource?.metadata?.name
    } for cn in _clusterNamesRef for s in _remoteSecrets \
      if cn == s.Resource?.metadata?.labels?["skycluster.io/cluster-name"]
} if _remoteSecrets else Undefined

# TODO: When the kubeconfig file is refreshed/rotated, 
# the current stored kubeconfig is not valid anymore.
_remoteK8SKubeconfigs = {
  cn = {
    kubeconfig = p?.Resource?.data?["kubeconfig"]
  } for cn in _clusterNamesRef for p in (extra?["kconfigSecrets"] or []) \
      if cn == p?.Resource?.metadata?.labels?["skycluster.io/cluster-name"]
}

_remoteK8SProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteK8SProviderCfgs \
      if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
}

_remoteHelmProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteHelmProviderCfgs \
      if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
}
assert len(_remoteK8SProviderCfgsMap) == len(_remoteHelmProviderCfgsMap), \
  "Remote K8S and Helm provider configs must exist for all clusters, found {} K8S and {} Helm configs."\
    .format(len(_remoteK8SProviderCfgsMap), len(_remoteHelmProviderCfgsMap))

# merge the two
_remoteProviderCfgsMaps = {
  cn = {
    k8s = _remoteK8SProviderCfgsMap[cn]
    helm = _remoteHelmProviderCfgsMap[cn]
  } for cn in _remoteHelmProviderCfgsMap
}

# Retrieve xkubes pod and service cidr
# will be used in the next step
_xkubesRefData = {
  o?.Resource?.status?.clusterName = {
    podCidr = o?.Resource?.status?.podCidr
    serviceCidr = o?.Resource?.status?.serviceCidr
    platform = o?.Resource?.spec?.providerRef?.platform
  } for s in _clusterNames for k, obj in extra if k == s for o in obj
} | {
  # merge with local management cluster's data
  k = {
    podCidr = oxr.spec?.localCluster?.podCidr
    serviceCidr = oxr.spec?.localCluster?.serviceCidr
  } for k in [_k8sMgmtClusterName]
}

_items = []

######################### Cleanup Istio and Submariner ############

#
# Submariner operator, using the secret above
# We already create istio-operator for the local kubernetes cluster
# so we skip creation for the local cluster
#

_chartsSubm = {
  label = "subm"
  version = "0.20.1"
  repo = "https://submariner-io.github.io/submariner-charts/charts"
  name = "submariner-operator"
  ns = "submariner-operator"
  blocking_obj = "Submariner/submariner" # # space-separated K8s object references
  prefix_obj = "submariner"
}

_chartIstio = {
  n = {
    label = n
    version = "1.27.0"
    repo = "https://istio-release.storage.googleapis.com/charts"
    name = n
    ns = "istio-system"
    prefix_obj = "istio"
    # space-separated K8s object references
    blocking_obj = " ".join(["CustomResourceDefinition/{}".format(s) for s in [
      "wasmplugins.extensions.istio.io", 
      "destinationrules.networking.istio.io", 
      "envoyfilters.networking.istio.io", 
      "gateways.networking.istio.io", 
      "proxyconfigs.networking.istio.io", 
      "serviceentries.networking.istio.io", 
      "sidecars.networking.istio.io", 
      "virtualservices.networking.istio.io", 
      "workloadentries.networking.istio.io",
      "authorizationpolicies.security.istio.io",
      "peerauthentications.security.istio.io",
      "requestauthentications.security.istio.io",
      "telemetries.telemetry.istio.io"
    ]])
  } for n in ["base", "istiod"]
}

_items += [
  *[_helper_cleanup_pod(s, _chartsSubm, _remoteK8SKubeconfigs, _k8sProvCfgName) for s in _clusterNamesRef if s != _k8sMgmtClusterName],
  *[_helper_cleanup_pod(s, _chartIstio["base"], _remoteK8SKubeconfigs, _k8sProvCfgName) for s in _clusterNamesRef],
  *[_helper_cleanup_pod(s, _chartIstio["istiod"], _remoteK8SKubeconfigs, _k8sProvCfgName) for s in _clusterNamesRef]
]

# There are two types of config status: cacerts-status, cleanup-status
# since configmap status are filtered based on oxr.metadata.name and this object is 
# set a their controller reference, logically there cannot be more than one of these objects
# (i.e. one for cacerts and one for cleanups)
_helper_filter = lambda cms, name {
  status = [cm.Resource for cm in cms if cm.Resource.metadata?.labels?["skycluster.io/result-type"] == name]?[0] \
    if cms else Undefined
  { "{}".format(s) = status?.data?[s] for s in _clusterNamesRef } if status else Undefined
}
_cleanupBase = _helper_filter(_cmsStatus, "cleanup-base")
_cleanupIstiod = _helper_filter(_cmsStatus, "cleanup-istiod")
_cleanupSubm = _helper_filter(_cmsStatus, "cleanup-submariner-operator")
_cleanupIstioReadiness = lambda s {
  False if not _cleanupBase or not _cleanupIstiod else \
    all_true([_cleanupBase[s] == "true", _cleanupIstiod[s] == "true"])
}
_cleanupSubmReadiness: (any) -> bool = lambda s {
  False if not _cleanupSubm else _cleanupSubm[s] == "true"
}
_cacertsStatus = _helper_filter(_cmsStatus, "cacerts-status")
_cacertsReadiness = lambda s {
  False if not _cacertsStatus else _cacertsStatus[s] == "true"
}
_remoteSecretsStatus = _helper_filter(_cmsStatus, "remote-secret-status")
_remoteSecretsReadiness = lambda s {
  False if not _remoteSecretsStatus else _remoteSecretsStatus[s] == "true"
}
# result-type "cacerts-status" or "cleanup-base" or "cleanup-istiod", "cleanup-submariner-operator", "remote-secret-status"


# # ###################### dxr ######################

dxr = {
  **option("params").dxr,
  status = {
    clusters = [{ 
      nameRef = s
      status = ""
    } for s in _clusterNamesRef]
    #   providerConfigName = _remoteK8SProviderCfgsMap[s]
    #   if _remoteSecretsMap:
    #     secretName = _remoteSecretsMap?[s]?.name or Undefined
    # } for s in _clusterNamesRef]
    # log = json.encode(_xkubesRefData)
  }
}

extraItems = {
  apiVersion = "meta.krm.kcl.dev/v1alpha1"
  kind = "ExtraResources"
  requirements = {
    **{s = {
        apiVersion: "skycluster.io/v1alpha1",
        kind: "XKube",
        matchName: s
      } for s in _clusterNames if s != _k8sMgmtClusterName
    }
    **{"submarinerSecret" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/config-type": "connection-secret"
        }
    }}
    **{"kconfigSecrets" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/secret-type": "skycluster-kubeconfig"
        }
    }}
    **{"cmsStatus" = {
        apiVersion: "v1",
        kind: "ConfigMap",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/config-type": "status-result",
          "skycluster.io/owner-uid": oxr.metadata?.uid
        }
    }}
    **{"clusterRemoteSecrets" = {
      apiVersion: "v1",
      kind: "Secret",
      matchLabels: {
        "skycluster.io/managed-by": "skycluster",
        "skycluster.io/secret-type": "cluster-cacert"
      }
    }}
  }
}

items = [*_items, dxr, extraItems]


_helper_cleanup_pod = lambda s, c, kc, k8scfg {
  k8sv1a2.Object{
    metadata = {
      labels = {
        "skycluster.io/pod-type": "cleanup-pod",
        "skycluster.io/cluster-name": "{}".format(s)
      }
      annotations = helper._set_resource_name("cl-{}-{}".format(c.label, s)) | {
        "skycluster.io/creation-time" = "{}".format(oxr.metadata?.creationTimestamp)
      }
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "batch/v1"
        kind = "Job"
        metadata = { 
          name = "cl-{}-{}".format(c.label, s),
          namespace = "skycluster-system",
          labels = {
            "skycluster.io/pod-type": "cleanup-pod",
            "skycluster.io/cluster-name": s
          },
        } 
        spec = {
          ttlSecondsAfterFinished = 120
          completions = 1
          backoffLimit = 0
          if ocds?["cl-{}-{}".format(c.label, s)]?.Resource?.spec?.forProvider?.manifest?.spec?.template:
            # reuse existing template if exists, for some reason, pod template spec ends up being modified
            template = ocds?["cl-{}-{}".format(c.label, s)]?.Resource?.spec?.forProvider?.manifest?.spec?.template
          else:
            template = {
              metadata = {
                labels = {
                  "skycluster.io/pod-type": "cleanup-pod",
                  "skycluster.io/cluster-name": s
                }
              }
              spec = {
                serviceAccountName = "skycluster-sva"
                restartPolicy = "Never"
                volumes = [{
                    name = "work"
                    emptyDir = {}
                  }, {
                    name = "script"
                    configMap = {
                      name = "script-kubemesh-setup"
                      defaultMode = 0755
                    }
                  }
                ]
                containers = [{
                    name = "runner"
                    image = "etesami/kubectl:latest"
                    imagePullPolicy = "Always"
                    command = ["/bin/bash","-lc","/script/cleanup.sh"]
                    env = [{
                        name = "CLUSTER_NAME"
                        value = s
                      }, {
                        name = "CHART_REPO"
                        value = c.repo
                      }, {
                        name = "CHART_NAME"
                        value = c.name
                      }, {
                        name = "CHART_VERSION"
                        value = c.version
                      },{
                        name = "CHART_NAMESPACE"
                        value = c.ns
                      }, {
                        name = "KUBECONFIG_B64"
                        value = kc[s]?["kubeconfig"] or "local"
                      }, {
                        # space-separated K8s object references
                        name = "BLOCKING_OBJECTS"
                        value = c.blocking_obj or Undefined
                      }, {
                        name = "CLUSTERROLE_PREFIX"
                        value = c.prefix_obj
                      }, {
                        name = "OWNER"
                        value = oxr.metadata.name
                      }, {
                        name = "OWNER_PATCH"
                        value = json.encode({
                          metadata = {
                            ownerReferences = [{
                              apiVersion = "skycluster.io/v1alpha1",
                              kind = "XKubeMesh",
                              name = oxr.metadata.name,
                              uid = oxr.metadata.uid,
                              controller = True,
                              blockOwnerDeletion = True
                            }]
                          }
                        })
                      }, {
                        name = "SCRIPTS_DIR"
                        value = "/script"
                      }
                    ]
                    volumeMounts = [{
                        name = "work"
                        mountPath = "/work"
                      }, {
                        name = "script"
                        mountPath = "/script"
                      }
                    ]
                  }
                ]
              }
            }
        }
      }
      providerConfigRef = {
        name = k8scfg
      }
    }
  }
}

