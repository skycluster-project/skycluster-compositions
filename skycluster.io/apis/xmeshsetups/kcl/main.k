
# XKubeMesh Setup

# import json

import helper.v1alpha1.main as helper
import provider_kubernetes.v1alpha2 as k8sv1a2

oxr = option("params").oxr # observed composite resource
ocds = option("params")?.ocds # observed composed resources
extra = option("params")?.extraResources
# _dxr = option("params").dxr # desired composite resource
# dcds = option("params").dcds # desired composed resources

ctx = option("params")?.ctx
assert ctx is not Undefined, "Context must be provided in the params"

_extraRes = ctx["apiextensions.crossplane.io/extra-resources"]
assert _extraRes is not Undefined, "Extra resources must be provided in the context"

_xsetup = _extraRes["XSetups"]?[0] or Undefined
assert _xsetup is not Undefined, "XSetup resource must be provided in the extra resources"

_cmsStatus = extra["cmsStatus"] or Undefined

# These are secrets containing istio remote secrets for remote clusters
# They will be generated by the Job later here
_remoteSecrets = extra["clusterRemoteSecrets"] or Undefined

_remoteK8SProviderCfgs = _extraRes["ClusterK8SProviderConfigs"] or Undefined
assert _remoteK8SProviderCfgs is not Undefined, "ClusterK8SProviderConfigs resource must be provided in the extra resources"

_remoteHelmProviderCfgs = _extraRes["ClusterHelmProviderConfigs"] or Undefined
assert _remoteHelmProviderCfgs is not Undefined, "ClusterHelmProviderConfigs resource must be provided in the extra resources"

assert oxr.spec?.localCluster?.podCidr, "Pod CIDR must be provided in the spec"
assert oxr.spec?.localCluster?.serviceCidr, "Service CIDR must be provided in the spec"

# Default (local) provider config
_k8sProvCfgName = _xsetup.status?.providerConfig?.kubernetes?.name or Undefined
_k8sMgmtClusterName = "skycluster-management"

_clusterNames = oxr.spec?.clusterNames or []
assert len(_clusterNames) > 0, "At least one cluster name must be provided in the spec.clusters"

#
# fetch referenced xkubes objects based on the cluster names (cloud-provider specific k8s)
#
_clusterNamesRef = [o?.Resource?.status?.clusterName for s in _clusterNames for k, obj in extra if k == s for o in obj]
# since we have the local management cluster as part of the multi-cluster setup
# we manually add the name of the local management cluster
_clusterNamesRef += [_k8sMgmtClusterName]

# construct helper maps for remote-secrets and remote-provider-configs
_remoteSecretsMap: {str:{str:any}} = {
  cn = {
      data = s.Resource?.data
      name = s.Resource?.metadata?.name
    } for cn in _clusterNamesRef for s in _remoteSecrets \
      if cn == s.Resource?.metadata?.labels?["skycluster.io/cluster-name"]
} if _remoteSecrets else Undefined

_remoteK8SKubeconfigs = {
  cn = {
    kubeconfig = p?.Resource?.data?["kubeconfig"]
  } for cn in _clusterNamesRef for p in (extra?["kconfigSecrets"] or []) \
      if cn == p?.Resource?.metadata?.labels?["skycluster.io/cluster-name"]
}

_remoteK8SProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteK8SProviderCfgs \
      if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
}

_remoteHelmProviderCfgsMap = {
  cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteHelmProviderCfgs \
      if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
}
assert len(_remoteK8SProviderCfgsMap) == len(_remoteHelmProviderCfgsMap), \
  "Remote K8S and Helm provider configs must exist for all clusters, found {} K8S and {} Helm configs."\
    .format(len(_remoteK8SProviderCfgsMap), len(_remoteHelmProviderCfgsMap))

# # merge the two
_remoteProviderCfgsMaps = {
  cn = {
    k8s = _remoteK8SProviderCfgsMap[cn]
    helm = _remoteHelmProviderCfgsMap[cn]
  } for cn in _remoteHelmProviderCfgsMap
}

# Retrieve xkubes pod and service cidr
# will be used in the next step
_xkubesRefData = {
  o?.Resource?.status?.clusterName = {
    podCidr = o?.Resource?.status?.podCidr
    serviceCidr = o?.Resource?.status?.serviceCidr
    platform = o?.Resource?.spec?.providerRef?.platform
  } for s in _clusterNames for k, obj in extra if k == s for o in obj
} | {
  # merge with local management cluster's data
  k = {
    podCidr = oxr.spec?.localCluster?.podCidr
    serviceCidr = oxr.spec?.localCluster?.serviceCidr
  } for k in [_k8sMgmtClusterName]
}

_items = []

#
# Dummy pod for submariner issue
#
_items += [
    # This is a dummy pod that is required for submariner to detect service cidr.
    # Submariner implement an outdated practice in determining service cidr where 
    # in our case it fails.
    _helper_dummy_pod_svc_cidr(s) for s in _clusterNamesRef \
      if s != _k8sMgmtClusterName 
] if _remoteK8SKubeconfigs else []
_items += [
    _helper_dummy_pod_cluster_cidr(s) for s in _clusterNamesRef \
      if s != _k8sMgmtClusterName
] if _remoteK8SKubeconfigs else []

# # routing for gcp
# _items += [
#     _helper_route_replicator_daemonset(s) for s in _clusterNamesRef \
#       if s != _k8sMgmtClusterName and _xkubesRefData[s]?.platform == "gcp"
# ] if _remoteK8SKubeconfigs else []

_items += [
    # This is a dummy pod that is required to apply rp_filter workaround for GCP clusters
    # to work with Submariner
    _helper_rp_filter_daemonset(s) for s in _clusterNamesRef \
      if _xkubesRefData[s]?.platform in ["gcp", "aws"]
] if _remoteK8SKubeconfigs else []

#
# Only for aws, enable IP forwarding using aws cli command
#
_items += [
  _helper_src_dst_check_daemonset(s) for s in _clusterNamesRef \
    if _xkubesRefData[s]?.platform == "aws" and _remoteProviderCfgsMaps \
      or ocds?["src-dst-check-daemon-{}".format(s)]
]
_items += [
  _helper_sync_routes_daemonset(s) for s in _clusterNamesRef \
    if _xkubesRefData[s]?.platform == "aws" and _remoteProviderCfgsMaps \
      or ocds?["sync-routes-daemon-{}".format(s)]
]

# # ###################### dxr ######################

dxr = {
  **option("params").dxr,
  # status = {
  #   clusters = [{ 
  #     nameRef = s
  #     status = ""
  #   } for s in _clusterNamesRef]
  #   # log = json.encode(_xkubesRefData)
  # }
}

extraItems = {
  apiVersion = "meta.krm.kcl.dev/v1alpha1"
  kind = "ExtraResources"
  requirements = {
    **{s = {
        apiVersion: "skycluster.io/v1alpha1",
        kind: "XKube",
        matchName: s
      } for s in _clusterNames if s != _k8sMgmtClusterName
    }
    **{"submarinerSecret" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/config-type": "connection-secret"
        }
    }}
    **{"kconfigSecrets" = {
        apiVersion: "v1",
        kind: "Secret",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/secret-type": "skycluster-kubeconfig"
        }
    }}
    **{"cmsStatus" = {
        apiVersion: "v1",
        kind: "ConfigMap",
        matchLabels: {
          "skycluster.io/managed-by": "skycluster",
          "skycluster.io/config-type": "status-result",
          "skycluster.io/owner-uid": oxr.metadata?.uid
        }
    }}
    **{"clusterRemoteSecrets" = {
      apiVersion: "v1",
      kind: "Secret",
      matchLabels: {
        "skycluster.io/managed-by": "skycluster",
        "skycluster.io/secret-type": "cluster-cacert"
      }
    }}
  }
}

items = [*_items, dxr, extraItems]


_helper_dummy_pod_svc_cidr = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "dummy-pod"}
      annotations = helper._set_resource_name("d-svc-cidr-{}".format(s))
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "Deployment"
        metadata = { 
          name = "d-svc-cidr-{}".format(s),
          namespace = "kube-system",
          labels = {
            "component": "kube-apiserver"
            "skycluster.io/pod-type": "dummy-pod"
          },
        }
        spec = {
          replicas = 1
          selector = {
            matchLabels = {
              "component": "kube-apiserver"
              "skycluster.io/pod-type": "dummy-pod"
            }
          }
          template = {
            metadata = {
              labels = {
                "component": "kube-apiserver"
                "skycluster.io/pod-type": "dummy-pod"
              }
            }
            spec = {
              containers = [{
                name = "dummy-container"
                image = "busybox"
                command = ["sh","-c","echo \"Starting dummy pod\";\n          tail -f /dev/null"]
                args = ["--service-cluster-ip-range={}".format(_xkubesRefData[s]?.serviceCidr)]  
              }]
            }
          }
        }
      }
      providerConfigRef = {
        # run the pod in the local cluster and use kubeconfig to configure remote cluster
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}

_helper_dummy_pod_cluster_cidr = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "dummy-pod"}
      annotations = helper._set_resource_name("d-cidr-{}".format(s))
    }
    spec =  {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "Deployment"
        metadata = { 
          name = "d-cidr-{}".format(s),
          namespace = "kube-system",
          labels = {
            "component": "kube-controller-manager"
            "skycluster.io/pod-type": "dummy-pod"
          },
        }
        spec = {
          replicas = 1
          selector = {
            matchLabels = {
              "component": "kube-controller-manager"
              "skycluster.io/pod-type": "dummy-pod"
            }
          }
          template = {
            metadata = {
              labels = {
                "component": "kube-controller-manager"
                "skycluster.io/pod-type": "dummy-pod"
              }
            }
            spec = {
              containers = [{
                name = "dummy-container"
                image = "busybox"
                command = ["sh","-c","echo \"Starting dummy pod\";\n          tail -f /dev/null"]
                args = ["--cluster-cidr={}".format(_xkubesRefData[s]?.podCidr)]
              }]
            }
          }
        }
      }
      providerConfigRef = {
        # run the pod in the local cluster and use kubeconfig to configure remote cluster
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}

_helper_rp_filter_daemonset = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "rp-filter"}
      annotations = helper._set_resource_name("rp-filter-{}".format(s))
    }
    spec = {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "DaemonSet"
        metadata = {
          name = "rp-filter-{}".format(s),
          namespace = "kube-system",
          labels = {
            "component": "kube-apiserver"
            "skycluster.io/pod-type": "rp-filter"
          },
        }
        spec = {
          selector.matchLabels = {
            "name": "rp-filter-{}".format(s)
          }
          template = {
            metadata.labels = {
              "name": "rp-filter-{}".format(s)
            }
            spec = {
              hostNetwork = True
              restartPolicy = "Always"
              containers = [{
                name = "netshoot-hostmount"
                image = "nicolaka/netshoot"
                args = ["/bin/bash","-c","echo 2 > /proc/sys/net/ipv4/conf/all/rp_filter; sysctl net.ipv4.conf.all.rp_filter; tail -f /dev/null"]
                stdin = True
                stdinOnce = True
                terminationMessagePath = "/dev/termination-log"
                terminationMessagePolicy = "File"
                tty = True
                securityContext = {
                  allowPrivilegeEscalation = True
                  privileged = True
                  runAsUser = 0
                  capabilities = {
                    add = ["ALL"]
                  }
                }
                volumeMounts = [{
                  mountPath = "/host"
                  name = "host-slash"
                  readOnly = True
                }]
              }]
              volumes = [{
                hostPath = {
                  path = "/"
                  type = ""
                }
                name = "host-slash"
              }]
            }
          }
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}

_helper_sync_routes_daemonset = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "sync-routes"}
      annotations = helper._set_resource_name("sync-routes-{}".format(s))
    }
    spec = {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "DaemonSet"
        metadata = {
          name = "sync-routes-{}".format(s),
          namespace = "kube-system",
          labels = {
            "skycluster.io/pod-type": "sync-routes"
          },
        }
        spec = {
          selector.matchLabels = {
            "name": "sync-routes-{}".format(s)
          }
          template = {
            metadata.labels = {
              "name": "sync-routes-{}".format(s)
            }
            spec = {
              hostNetwork = True
              restartPolicy = "Always"
              containers = [{
                name = "netshoot-hostmount"
                image = "nicolaka/netshoot"
                args = ["/bin/bash","-c", _helper_sync_table_2_with_main]
                stdin = True
                stdinOnce = True
                terminationMessagePath = "/dev/termination-log"
                terminationMessagePolicy = "File"
                tty = True
                securityContext = {
                  allowPrivilegeEscalation = True
                  privileged = True
                  runAsUser = 0
                  capabilities = {
                    add = ["ALL"]
                  }
                }
                volumeMounts = [{
                  mountPath = "/host"
                  name = "host-slash"
                  readOnly = True
                }]
              }]
              volumes = [{
                hostPath = {
                  path = "/"
                  type = ""
                }
                name = "host-slash"
              }]
            }
          }
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}

_helper_src_dst_check_daemonset = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "src-dst-check"}
      annotations = helper._set_resource_name("src-dst-check-{}".format(s))
    }
    spec = {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "DaemonSet"
        metadata = {
          name = "src-dst-check-{}".format(s),
          namespace = "kube-system",
          labels = {
            "skycluster.io/pod-type": "src-dst-check"
          },
        }
        spec = {
          selector.matchLabels = {
            "name": "src-dst-check-{}".format(s)
          }
          template = {
            metadata.labels = {
              "name": "src-dst-check-{}".format(s)
            }
            spec = {
              hostNetwork = True
              restartPolicy = "Always"
              containers = [{
                name = "aws-cli"
                image = "amazon/aws-cli:2.15.40"
                command: ["/bin/bash"]
                args: ["-c", _helper_script_src_dst_check]
                stdin = True
                stdinOnce = True
                terminationMessagePath = "/dev/termination-log"
                terminationMessagePolicy = "File"
                tty = True
                securityContext = {
                  allowPrivilegeEscalation = True
                  privileged = True
                  runAsUser = 0
                  capabilities = {
                    add = ["ALL"]
                  }
                }
                volumeMounts = [
                  {
                  mountPath = "/host"
                  name = "host-slash"
                  readOnly = True
                  }
                ]
              }]
              volumes = [
                {
                  hostPath = {
                    path = "/"
                    type = ""
                  }
                  name = "host-slash"
                }
              ]
            }
          }
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}

_helper_route_replicator_daemonset = lambda s {
  k8sv1a2.Object{
    metadata = {
      labels = {"skycluster.io/pod-type": "route-replicator"}
      annotations = helper._set_resource_name("route-replicator-{}".format(s))
    }
    spec = {
      forProvider.manifest = {
        apiVersion = "apps/v1"
        kind = "DaemonSet"
        metadata = {
          name = "route-replicator-{}".format(s),
          namespace = "kube-system",
          labels = {
            "component": "route-replicator"
            "skycluster.io/pod-type": "route-replicator"
            "role": "route-replicator"
          },
        }
        spec = {
          selector.matchLabels = {
            "name": "route-replicator-{}".format(s)
          }
          template = {
            metadata.labels = {
              "name": "route-replicator-{}".format(s)
              "role": "route-replicator"
            }
            spec = {
              nodeSelector = {
                "submariner.io/gateway" = "true"
              }
              hostNetwork = True
              restartPolicy = "Always"
              containers = [{
                name = "route-replicator"
                image = "alpine:3.18"
                args = ["/bin/sh","-c", _helper_script_route_setup.replace("__POD_CIDR__", _xkubesRefData[s]?.podCidr).replace("__SVC_CIDR__", _xkubesRefData[s]?.serviceCidr)]
                terminationMessagePath = "/dev/termination-log"
                terminationMessagePolicy = "File"
                securityContext = {
                  runAsUser = 0
                  capabilities = {
                    add = ["NET_ADMIN"]
                  }
                }
              }]              
            }
          }
        }
      }
      providerConfigRef = {
        name = _remoteProviderCfgsMaps[s]?.k8s
      }
    }
  }
}


_helper_script_route_setup = """\
#!/bin/sh
set -eu

apk add --no-cache iproute2 >/dev/null 2>&1 || true

TABLE_NAME=skycluster
TABLE_ID=200
IFF=submariner
PRIO=2000

CIDR1=__POD_CIDR__
CIDR2=__SVC_CIDR__

# ensure table mapping exists in /etc/iproute2/rt_tables
if ! grep -qE "^[[:space:]]*$TABLE_ID[[:space:]]+$TABLE_NAME$" /etc/iproute2/rt_tables; then
  echo "$TABLE_ID $TABLE_NAME" >> /etc/iproute2/rt_tables
fi

# ensure rule for each CIDR
for cidr in "$CIDR1" "$CIDR2"; do
  if ! ip rule show | grep -q -E "iif $IFF.*to +$cidr.*lookup $TABLE_NAME|to +$cidr.*iif $IFF.*lookup $TABLE_NAME"; then
    ip rule add iif "$IFF" to "$cidr" lookup "$TABLE_NAME" priority "$PRIO" || true
  fi
done

# replicate main table (excluding default) into the skycluster table continuously
while true; do
  ip route flush table "$TABLE_NAME" 2>/dev/null || true
  ip route show table main | grep -v '^default' | while IFS= read -r line; do
    [ -z "$line" ] && continue
    ip route add table "$TABLE_NAME" $line 2>/dev/null || true
  done
  sleep 10
done
"""

_helper_script_src_dst_check = """\
#!/bin/sh
set -e

TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" --connect-timeout 5 -m 5)
if [ -z "$TOKEN" ]; then
  echo "Failed to get AWS metadata token"
  exit 1
fi

# use token to get instance-id
INSTANCE_ID=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/instance-id)
if [ -z "$INSTANCE_ID" ]; then
  echo "Failed to get instance ID"
  exit 1
fi

echo "Instance ID: $INSTANCE_ID"

while true; do
  ENI_IDS=$(aws ec2 describe-instances \
    --instance-ids "$INSTANCE_ID" \
    --query "Reservations[].Instances[].NetworkInterfaces[].NetworkInterfaceId" \
    --output text)

  for eni in $ENI_IDS; do
    echo "Disabling source-dest-check on ENI: $eni"
    aws ec2 modify-network-interface-attribute \
      --network-interface-id "$eni" \
      --no-source-dest-check || true
  done

  echo "Iteration complete. Sleeping 60 seconds..."
  sleep 60
done
"""

_helper_sync_table_2_with_main = """\
#!/usr/bin/env bash
set -euo pipefail

TABLE_MAIN=main
TABLE_TARGET=2
MATCH_REGEX='vx-submariner|via 240\.'
SLEEP=10

log() {
  echo "$(date -Is) $*"
}

sync_routes() {
  tmp_main=$(mktemp)
  tmp_target=$(mktemp)
  tmp_main_prefixes=$(mktemp)
  tmp_target_prefixes=$(mktemp)

  ip -4 route show table $TABLE_MAIN | grep -E "$MATCH_REGEX" > $tmp_main 2>/dev/null || true
  awk '{print $1}' $tmp_main > $tmp_main_prefixes

  while IFS= read -r line || [ -n "$line" ]; do
    if [ -z "$line" ]; then
      continue
    fi

    log "Processing: $line"

    # Primary sanitize: remove "proto static"
    sanitized=$(printf '%s\n' "$line" | sed -E 's/ proto static//g')

    log "Trying: ip route replace $sanitized table $TABLE_TARGET"
    if ip route replace $sanitized table $TABLE_TARGET 2>/dev/null; then
      continue
    fi

    # Fallback sanitize: remove other kernel tokens that break "ip route replace"
    sanitized2=$(printf '%s\n' "$sanitized" | sed -E 's/ proto kernel//g; s/ scope link//g; s/ src [0-9]+\.[0-9]+\.[0-9]+\.[0-9]+//g')

    log "Trying: ip route replace $sanitized2 table $TABLE_TARGET"
    if ip route replace $sanitized2 table $TABLE_TARGET 2>/dev/null; then
      continue
    fi

    log "ip route replace failed for lines: original='$line' sanitized='$sanitized' fallback='$sanitized2'"
  done < $tmp_main

  # Cleanup stale entries from target table
  ip -4 route show table $TABLE_TARGET | grep -E "$MATCH_REGEX" > $tmp_target 2>/dev/null || true
  awk '{print $1}' $tmp_target > $tmp_target_prefixes

  while IFS= read -r tprefix || [ -n "$tprefix" ]; do
    if [ -z "$tprefix" ]; then
      continue
    fi
    if ! grep -Fxq "$tprefix" $tmp_main_prefixes; then
      log "Deleting stale route from table $TABLE_TARGET: $tprefix"
      ip route del "$tprefix" table $TABLE_TARGET 2>/dev/null || log "ip route del failed for: $tprefix"
    fi
  done < $tmp_target_prefixes

  rm -f $tmp_main $tmp_target $tmp_main_prefixes $tmp_target_prefixes
}

log "Starting submariner route sync (main -> table $TABLE_TARGET), interval $SLEEP"
while true; do
  if ! sync_routes; then
    log "sync error"
  fi
  sleep $SLEEP
done
"""