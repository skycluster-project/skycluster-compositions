import yaml
import json
import base64
import helper.v1alpha1.main as helper
import provider_kubernetes.v1alpha2 as k8sv1a2

oxr = option("params").oxr # observed composite resource
ocds = option("params")?.ocds # observed composed resources
extra = option("params")?.extraResources
# _dxr = option("params").dxr # desired composite resource
# dcds = option("params").dcds # desired composed resources

_default_labels = {
  **oxr.metadata?.labels,
  "skycluster.io/managed-by" = "skycluster"
}
_default_annotations = {**oxr.metadata?.annotations}

assert oxr.metadata?.labels is not Undefined, "At least one label must be specified"
assert "skycluster.io/managed-by" in oxr.metadata.labels, "Label 'skycluster.io/managed-by' must be specified"

ctx = option("params")?.ctx
assert ctx is not Undefined, "Context must be provided in the params"

_extraRes = ctx["apiextensions.crossplane.io/extra-resources"]
assert _extraRes is not Undefined, "Extra resources must be provided in the context"

_cms = _extraRes["ConfigMaps"][0]
_skySetup = _extraRes["SkySetups"][0]
_k8sProvCfgName = _skySetup.status?.providerConfig?.kubernetes?.name or Undefined
_caCertificateB64 = _skySetup.status?.ca.certificate or Undefined
assert _k8sProvCfgName and _caCertificateB64, "Kubernetes provider config name and CA certificate must be specified in the SkySetup status"
_loginUrl = _skySetup.status?.headscale?.loginUrl 
assert _loginUrl, "Headscale login URL must be specified in the SkySetup status"
_hsToken = _skySetup.status?.headscale?.token
assert _hsToken, "Headscale token must be specified in the SkySetup status"

# Gateway script for setting up tailscale and overlay network
_gwScript = _extraRes["Scripts"][0]

_tsInstallProbSc = _gwScript?.data?["tailscale_installation_prob.sh"] or Undefined  
_tsInstallEnsureSc = _gwScript?.data?["tailscale_installation.sh"] or Undefined

# tailscale installation
_envStaticScript = _gwScript?.data?["env_prepare_static_script.sh"] or Undefined
_envStaticProbScript = _gwScript?.data?["env_prepare_static_prob_script.sh"] or Undefined

# tailscale run script, needs variables replaced
_tsRunScript = _gwScript?.data?["tailscale_run_script.sh"] or Undefined
_tsRunProbScript = _gwScript?.data?["tailscale_run_prob_script.sh"] or Undefined

_scriptGate = all_true([
  _tsRunScript, _tsRunProbScript,
  _envStaticScript, _envStaticProbScript,
  _tsInstallEnsureSc, _tsInstallProbSc,
])
assert _scriptGate, "All gateway scripts must be provided in the context"

_ns = _skySetup.spec.namespace or "skycluster-system"

_oxrProvRegion = oxr.spec.providerRef.region
_oxrProvZone = oxr.spec.providerRef.zones?.primary
_oxrProvPlatform = oxr.spec.providerRef.platform
_oxrAppId = oxr.spec.applicationId or Undefined

assert _oxrProvRegion and _oxrProvZone and _oxrProvPlatform, "Provider region, primary zone, platform must be specified"

assert oxr.spec?.gatewayDeviceName, "Gateway device name must be specified"

_gatewayNodeStr = _cms.data?["gateway"] or Undefined
_workerNodesStr = _cms.data?["worker"] or Undefined

_gwNodes = yaml.decode(_gatewayNodeStr) if _gatewayNodeStr else Undefined
_workerNodes = yaml.decode(_workerNodesStr) if _workerNodesStr else []

#
# Only supporting one gateway node for now
#
_gwNodeName = oxr.spec?.gatewayDeviceName
assert _gwNodeName, "Gateway device name must be specified"
assert _gwNodeName in _gwNodes, "Gateway device name '{}' not found in the configmap".format(_gwNodeName)

_workerNodeNames = oxr.spec?.workerDeviceNames or []
assert all_true([n in _workerNodesStr for n in _workerNodeNames]), \
  "Some worker device names not found in the configmap"

# Retrieve secret containing the private key for gateway and workers
_gwPrivateKeySecretName = _gwNodes?[_gwNodeName]?.auth?.privateKeySecretRef?.name
_gwPrivateKeySecretKey = _gwNodes?[_gwNodeName]?.auth?.privateKeySecretRef?.key

_gwSecrets = extra?["bmPrivateKeySecrets"]
_gwSecretData = [_sec.Resource.data for _sec in _gwSecrets \
  if _sec.Resource.metadata?.name == _gwPrivateKeySecretName] \
    if _gwSecrets and _gwPrivateKeySecretName else []

_gwPrivateKeyB64 = _gwSecretData?[0]?[_gwPrivateKeySecretKey] or Undefined
# Decrypted private key
_defaultPrivateKey = base64.decode(_gwPrivateKeyB64) if _gwPrivateKeyB64 else Undefined

#
# Fetch node spec and create a map with additional fields
#
_gwNodesMap = {
  n = {
    **_gwNodes[n]
    sshProviderCfgName = ocds?["ssh-pcfg-gw-{}".format(n)]?.Resource?.status?.atProvider?.manifest?.metadata?.name
    sshSecretName = ocds?["sec-ssh-gw-{}".format(n)]?.Resource?.status?.atProvider?.manifest?.metadata?.name
    sshGate = all_true([_gwNodes[n]?.publicIp, _defaultPrivateKey, _k8sProvCfgName])
    tailscaleGate = all_true([
      helper._ready(ocds?["ssh-gw-overlay-{}".format(n)]),
      helper._ready(ocds?["ssh-gw-env-{}".format(n)])
    ])
  } for n in [_gwNodeName] if _gwNodes and n in _gwNodes
}

#
# Worker nodes Map
#
_workerNodesMap: {str:any} = {
  n = {
    **_workerNodes[n],
    sshProviderCfgName = ocds?["ssh-pcfg-{}".format(n)]?.Resource?.status?.atProvider?.manifest?.metadata?.name
    sshSecretName = ocds?["sec-ssh-{}".format(n)]?.Resource?.status?.atProvider?.manifest?.metadata?.name
    sshGate = all_true([_workerNodes[n]?.privateIp, _defaultPrivateKey, _k8sProvCfgName])
  } for n in _workerNodeNames if _workerNodes and n in _workerNodes
}


_subnetCidr = _gwNodesMap?[_gwNodeName]?.privateIp
_subnetCidrOctets = _subnetCidr.split(".")
_subnetCidrFirst = _subnetCidrOctets[0]
_subnetCidrSecond = _subnetCidrOctets[1]
_subnetCidrThird = _subnetCidrOctets[2]
_subnetCidr = "{}.{}.{}.0/24".format(_subnetCidrFirst, _subnetCidrSecond, _subnetCidrThird)



# Start assembling resources
_items = []

#
# For [gateway] node, we create a secret and provider config
# By default, since there is only one gateway node name, one will be created
#
_items += [
  _helper_secret_remote("sec-ssh-gw-{}".format(n), spec.publicIp, spec.auth.username, _defaultPrivateKey) \
    for n, spec in _gwNodesMap if spec.sshGate or ocds?["sec-ssh-gw-{}".format(n)]
]

_items += [
  _helper_provider_cfg("ssh-pcfg-gw-{}".format(n), spec.publicIp, spec.sshSecretName) \
    for n, spec in _gwNodesMap if spec.sshSecretName or ocds?["ssh-pcfg-gw-{}".format(n)]
]

# 
# Create secrets and provider configs for [worker] nodes as well
#
_items += [
  _helper_secret_remote("sec-ssh-{}".format(n), spec.privateIp, spec.auth.username, _defaultPrivateKey)
  for n, spec in _workerNodesMap \
    if spec.sshGate or ocds?["sec-ssh-{}".format(n)]
]

# Create SSH provider configs for worker nodes
_items += [
  _helper_provider_cfg("ssh-pcfg-{}".format(n), spec.privateIp, spec.sshSecretName)
  for n, spec in _workerNodesMap \
    if spec.sshGate or ocds?["ssh-pcfg-{}".format(n)]
]

#  
# Tailscale installation
# It uses SSH provider config created above
#
_items += [
  _helper_ssh_task("ssh-gw-overlay-{}".format(n), spec.sshProviderCfgName, _tsInstallProbSc, _tsInstallEnsureSc) \
    for n, spec in _gwNodesMap \
      if (_scriptGate and spec.sshProviderCfgName) or ocds?["ssh-gw-overlay-{}".format(n)]
]

#
# Create some static environment setup such as creating the CA certificates
# These files are static and once created, normally they do not need to be re-created
#
_envStaticScript = _envStaticScript.replace("__SUBNETCIDR__", _subnetCidr)\
  .replace("__CA_CERTIFICATE_ENCODED__", _caCertificateB64)
_items += [
  _helper_ssh_task("ssh-gw-env-{}".format(n), spec.sshProviderCfgName, _envStaticProbScript, _envStaticScript) \
    for n, spec in _gwNodesMap \
      if (_scriptGate and spec.sshProviderCfgName) or ocds?["ssh-gw-env-{}".format(n)]
]

#
# Once ready a script runs the tailscale to join the overlay network
#
_tsRunScript = _tsRunScript.replace("__SUBNETCIDR__", _subnetCidr)\
  .replace("__OVERLAY_LOGIN_URL__", _loginUrl)\
  .replace("__OVERLAY_TOKEN__", _hsToken)\
  .replace("__OVERLAY_HOSTNAME__", oxr.metadata?.name)

# If tailscale installation is done, then we can run the environment setup
_items += [
  _helper_ssh_task("ssh-gw-run-{}".format(n), spec.sshProviderCfgName, _tsRunProbScript, _tsRunScript) \
    for n, spec in _gwNodesMap \
      if (_scriptGate and spec.sshProviderCfgName and spec.tailscaleGate) or ocds?["ssh-gw-run-{}".format(n)]
]



extraItems = {
  apiVersion = "meta.krm.kcl.dev/v1alpha1"
  kind = "ExtraResources"
  requirements = {
    **{"bmPrivateKeySecrets" = {
        apiVersion: "v1",
        kind: "Secret",
        # we cannot use matchName because the secret is namespaced
        # hence we filter by label instead and check the name later
        matchLabels = {
          "skycluster.io/managed-by" = "skycluster",
          "skycluster.io/secret-type" = "onpremise-keypair"
        }
    }}
  }
}

dxr = {
  **option("params").dxr,
  status = {
    # log = json.encode({
    #   # prob = _envSetupProbScript
    #   ensure = _envEnsureScript
    # })
    subnetCidr = _subnetCidr,
    gateway = {
      publicIp = _gwNodesMap?[_gwNodeName]?.publicIp,
      privateIp = _gwNodesMap?[_gwNodeName]?.privateIp,
      username = _gwNodesMap?[_gwNodeName]?.auth?.username,
    }
    auth = {
      secretName = _gwPrivateKeySecretName or Undefined
      # privateKey = _defaultPrivateKey or Undefined
    } if _defaultPrivateKey else Undefined
  }
}

# Collect all resources into a list for output
items = [*_items, dxr, extraItems]


# 
# Create SSH secret for a DeviceNode
# 
_helper_secret_remote = lambda s, ip, user, pvKey {
  k8sv1a2.Object{
    "metadata": {
      labels = _default_labels
      annotations = _default_annotations | {
        **helper._set_resource_name(s),
      },
    },
    spec = {
      forProvider = {
        manifest = {
          apiVersion: "v1",
          kind: "Secret",
          metadata: {
            name: "ssh-bm-{}-{}".format(_oxrProvRegion.lower(), ip),
            namespace: _ns,
            labels: _default_labels,
            annotations: _default_annotations,
          },
          type = "Opaque",
          stringData = {
            config = json.encode({
              username = user
              password = ""
              hostIP = ip
              hostPort = "22"
              privateKey = pvKey
        })}},
      }
      providerConfigRef.name = _k8sProvCfgName
    }
  }
}

# 
# Create SSH ProviderConfig for a DeviceNode
# 
_helper_provider_cfg = lambda s, ip, sshSecName {
  k8sv1a2.Object{
    "metadata" = {
      labels = _default_labels
      annotations = _default_annotations | {
        **helper._set_resource_name(s),
      },
    },
    spec = {
      references = [{
        # TODO: When object is deleted, the providerconfig remains
        # Manual deletion of provider config does not remove the finalizer on the secret
        # The owner of secret still remains waiting for secret to be removed.
        dependsOn = {
          apiVersion = "v1"
          kind = "Secret"
          name = "ssh-bm-{}-{}".format(_oxrProvRegion.lower(), ip),
          namespace = _ns
        }
      }]
      deletionPolicy = "Delete"
      forProvider = {
        manifest = {
          apiVersion = "ssh.crossplane.io/v1alpha1",
          kind = "ProviderConfig",
          metadata = {
            name = "ssh-bm-{}-{}".format(_oxrProvRegion.lower(), ip),
            namespace = _ns,
            labels = _default_labels,
            annotations = _default_annotations,
          },
          spec = {
            credentials = {
              source = "Secret"
              secretRef = {
                name = sshSecName
                namespace = _ns,
                key = "config"
        }}}},
      },
      providerConfigRef.name = _k8sProvCfgName
    }
  }
}

# Helper to install and setup tailscale
_helper_ssh_task = lambda s, pvCfgName, pbScript, ensScript {
  {
    apiVersion = "ssh.crossplane.io/v1alpha1"
    kind = "SSHTask"
    metadata = {
      labels = _default_labels
      annotations = _default_annotations | {
        **helper._set_resource_name(s),
      },
    },
    spec = {
      providerConfigRef.name = pvCfgName
      forProvider = {
        scripts = {
          probeScript.inline = pbScript
          ensureScript.inline = ensScript
        }
        observe = {
          refreshPolicy = "Always"
          capture = "both"
          "map" = []
        }
        execution = {
          sudo = True
          shell = "/bin/bash -euo pipefail"
          timeoutSeconds = 600
          # TODO: change and check setting to 10 to improve speed of convergence
          maxAttempts = 2
        }
        artifactPolicy.capture = "both"    
      }
    }
  }
}